# Default Spring Boot properties
spring.application.name=ai_research

# Agent Card Configuration (can be overridden by environment variables)
agent.card.id=${AGENT_CARD_ID:-ai-research-agent-2025}
agent.card.name=${AGENT_CARD_NAME:-Research Assistant Agent}
agent.card.description=${AGENT_CARD_DESCRIPTION:-An AI agent that helps with research tasks and information retrieval.}
agent.card.url=${AGENT_CARD_URL:-http://localhost:8080/research-agent/api}
agent.card.provider.organization=${AGENT_CARD_PROVIDER_ORGANIZATION:-AI Research Lab}
agent.card.provider.url=${AGENT_CARD_PROVIDER_URL:-https://example.com}
agent.card.contact_email=${AGENT_CARD_CONTACT_EMAIL:-contact@example.com}

# H2 Database Configuration (can be overridden by environment variables)
spring.datasource.url=${SPRING_DATASOURCE_URL:jdbc:h2:./data/airesearch}
spring.datasource.driverClassName=org.h2.Driver
spring.datasource.username=${SPRING_DATASOURCE_USERNAME:sa}
spring.datasource.password=${SPRING_DATASOURCE_PASSWORD:password}
spring.jpa.database-platform=org.hibernate.dialect.H2Dialect
spring.jpa.hibernate.ddl-auto=${SPRING_JPA_HIBERNATE_DDL_AUTO:update}
spring.h2.console.enabled=${SPRING_H2_CONSOLE_ENABLED:true}
spring.h2.console.path=${SPRING_H2_CONSOLE_PATH:/h2-console}

# External MCP Servers
agent.integrations.mcp-servers[0].name=webcrawl-mcp
agent.integrations.mcp-servers[0].url=http://webcrawl-mcp:3000

# OpenAI API Configuration (MUST be overridden by environment variables for security)
openai.api.baseurl=${OPENAI_API_BASEURL:http://127.0.0.1:1234/v1}
openai.api.key=${OPENAI_API_KEY:YOUR_OPENAI_API_KEY_HERE}
openai.api.model=${OPENAI_API_MODEL:qwen_qwen3-14b}
openai.api.systemRole=${OPENAI_API_SYSTEM_ROLE:You are an efficient research assistant with access to powerful tools. Follow these guidelines: 1) Analyze each request and determine if tools are needed - use the most appropriate tool(s) for the task; 2) Combine multiple tools when a single tool is insufficient; 3) Provide concise, direct answers without unnecessary text; 4) Never invent information - if uncertain, clearly state your limitations and suggest using tools to find accurate information; 5) Explicitly mention when you're using tools and explain your reasoning; 6) Present information clearly and organize complex responses logically; 7) If no available tools can answer a question, acknowledge this limitation honestly rather than guessing; 8) Prioritize speed and accuracy in your responses.}

# File Upload Configuration
# Set to accommodate the largest file type we support (PDFs up to 20MB)
spring.servlet.multipart.max-file-size=${MAX_FILE_SIZE:25MB}
spring.servlet.multipart.max-request-size=${MAX_REQUEST_SIZE:30MB}

# LLM Configurations - defines which models are available and their capabilities
# This allows the backend to understand which models support multimodal inputs
# The frontend can fetch this information to provide appropriate UI and warnings

# Qwen 3 (text-only model with tool use support)
llm.configurations[0].id=${LLM_CONFIG_0_ID:qwen_qwen3-14b}
llm.configurations[0].name=${LLM_CONFIG_0_NAME:Qwen 3 14B}
llm.configurations[0].supportsText=${LLM_CONFIG_0_SUPPORTS_TEXT:true}
llm.configurations[0].supportsImage=${LLM_CONFIG_0_SUPPORTS_IMAGE:false}
llm.configurations[0].supportsPdf=${LLM_CONFIG_0_SUPPORTS_PDF:false}
llm.configurations[0].notes=${LLM_CONFIG_0_NOTES:Text-only model, no multimodal capabilities but tooluse supported}

# Google Gemma 3 (vision-enabled model with image support)
llm.configurations[1].id=${LLM_CONFIG_1_ID:google_gemma-3-12b-it@q4_k_s}
llm.configurations[1].name=${LLM_CONFIG_1_NAME:Google Gemma 3 12B}
llm.configurations[1].supportsText=${LLM_CONFIG_1_SUPPORTS_TEXT:true}
llm.configurations[1].supportsImage=${LLM_CONFIG_1_SUPPORTS_IMAGE:true}
llm.configurations[1].supportsPdf=${LLM_CONFIG_1_SUPPORTS_PDF:false}
llm.configurations[1].notes=${LLM_CONFIG_1_NOTES:Supports image analysis up to 5MB per image}

# DeepSeek R1 0528 Qwen3 8B (text and reasoning model)
llm.configurations[2].id=${LLM_CONFIG_2_ID:deepseek-r1-0528-qwen3-8b}
llm.configurations[2].name=${LLM_CONFIG_2_NAME:DeepSeek R1 0528 Qwen3 8B}
llm.configurations[2].supportsText=${LLM_CONFIG_2_SUPPORTS_TEXT:true}
llm.configurations[2].supportsImage=${LLM_CONFIG_2_SUPPORTS_IMAGE:false}
llm.configurations[2].supportsPdf=${LLM_CONFIG_2_SUPPORTS_PDF:false}
llm.configurations[2].notes=${LLM_CONFIG_2_NOTES: Text-only model, no multimodal capabilities but reasoning supported}
